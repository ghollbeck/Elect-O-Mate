{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVEP4gHRmDMe"
      },
      "source": [
        "If you want to save everything: `Click on File -> Save a Copy in Drive`\n",
        "\n",
        "Now you have it in your drive and can always get back to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR0OTR0zK-1x"
      },
      "source": [
        "# Step 1 - Download the dependencies and the PDF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NtPU1dYfdGi",
        "outputId": "8c49b8a7-0b45-4ef0-8e07-d241742e1904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# The ! at the beginning is a jupyter notebook specific. It tells the server to execute this as\n",
        "# shell (instead of python) code.\n",
        "# pip : package installer for Python\n",
        "\n",
        "!pip install -q pymupdf\n",
        "!pip install -q langchain_community\n",
        "!pip install -q chromadb\n",
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3SwyCOmMHY0"
      },
      "outputs": [],
      "source": [
        "# Create folders to store the images and the Markdown files\n",
        "# mkdir: create a new folder\n",
        "\n",
        "!mkdir image_folder\n",
        "!mkdir md_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcnZCDn687Jl"
      },
      "outputs": [],
      "source": [
        "# wget: program to download\n",
        "# unzip: program to unzip (-o tells to overwrite if files exist)\n",
        "\n",
        "!wget -q https://github.com/eth-student-project-house/ws-ml-pdf/archive/refs/heads/main.zip\n",
        "!unzip -q -o /content/main.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61S_tpkL9jYK"
      },
      "source": [
        "You can also use your own PDF documents in Drive. Just uncomment and run the next cell so that this file has access to your Drive. We won't have access to your drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoCe5gqcjiVr"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9vaB11WLH1J"
      },
      "source": [
        "#Step 2 - Extract the Text and the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWuGbEGpPTEU"
      },
      "outputs": [],
      "source": [
        "pdf_file_folder = \"/content/ws-ml-pdf-main/\" #Uncomment this line if you use our PDF from Github\n",
        "#pdf_file_folder = \"/content/drive/MyDrive/...\"  #Uncomment this line if you use your own PDF from Drive\n",
        "pdf_name = \"olympic_history\" #@param {type:\"string\"}\n",
        "pdf_file_path = f\"{pdf_file_folder}{pdf_name}.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K7CbKVOhY6Z"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "from langchain_community.document_loaders import PyMuPDFLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDWthWoiPiVC"
      },
      "source": [
        "## Extract the text and save it in .md files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6BJ6iPPi41K"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(pdf_file_path)\n",
        "data = loader.load() #extract the text in raw format\n",
        "\n",
        "for page in data:# iterate over pdf pages\n",
        "    output_path = f\"md_docs/{pdf_name}_page_{page.metadata['page']+1}_{page.metadata['total_pages']}.md\" # get the path for the new .md file\n",
        "    with open(output_path, 'w') as f: # open a new .md file\n",
        "        f.write(page.page_content) # write in the .md file the raw text\n",
        "        print(f\"Markdown for document {pdf_name}, page {page.metadata['page']+1}/{page.metadata['total_pages']} saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTYCSi6gnagy"
      },
      "source": [
        "## Extract the images as .png files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkxh1LWygcdg"
      },
      "outputs": [],
      "source": [
        "doc = fitz.open(pdf_file_path) # load the document\n",
        "\n",
        "for page_index in range(len(doc)): # iterate over pdf pages\n",
        "    page = doc[page_index] # get the page\n",
        "\n",
        "    image_list = page.get_images() # get the images in the page\n",
        "\n",
        "    # print the number of images found on the page\n",
        "    if image_list:\n",
        "        print(f\"Found {len(image_list)} images on page {page_index + 1}\")\n",
        "    else:\n",
        "        print(\"No images found on page\", page_index + 1)\n",
        "\n",
        "    for image_index, img in enumerate(image_list, start=1): # enumerate the image list\n",
        "        xref = img[0] # get the XREF of the image\n",
        "        pix = fitz.Pixmap(doc, xref) # create a Pixmap\n",
        "\n",
        "        if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
        "            pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "        output_path = f\"image_folder/{pdf_name}_page_{page_index+1}-image_{image_index}.png\"\n",
        "        pix.save(output_path) # save the image as png in image_folder\n",
        "        print(f\"Image {image_index} from page {page_index+1} in document {pdf_name} saved to {output_path}\")\n",
        "        pix = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lXJdBHLLmKv"
      },
      "source": [
        "# Step 3 - Extract the Images information using GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGLFpwDTIlGc"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Describe the following picture as precisely as you can.\n",
        "            It should contain all the information such that someone can recreate the image from the text explanation.\n",
        "            Convert tables to markdown tables. Describe charts as best you can.\n",
        "            If the picture contains people don't describe them, only what they are doing very briefly.\n",
        "            Don't interpret what you see, only describe, nothing else.\n",
        "            DO NOT return in a codeblock. Just return the raw text in markdown format.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr0Q2hlgr7iC"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "\n",
        "# this openai api key is only valid today. go here to get your own:\n",
        "# https://platform.openai.com/api-keys\n",
        "api_key =\"sk-proj-jYsmUBNYhrpO34u3Ah7ST3BlbkFJORWMsB6UTJsaySVFL5OI\"\n",
        "openai_client = OpenAI(api_key=api_key)\n",
        "\n",
        "def image_to_markdown(base64_image):\n",
        "    # Makes a calll to GPT-4-vision to process an image\n",
        "    response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": prompt},\n",
        "              {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"low\"}}\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens =  4096\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    # Write the text description of the image in a .md file\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww55KO0Ip_Ca"
      },
      "source": [
        "## Process the images and store the result in .md files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF_Pe2hzqfh3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "input_path = \"image_folder\"\n",
        "output_folder = \"md_docs\"\n",
        "\n",
        "images = sorted(Path(input_path).iterdir(), key=lambda x: x.stem) # get all the images in the image_folder\n",
        "for image_path in images: # Iterate thourgh the images\n",
        "    output_path = Path(output_folder) / f\"{image_path.stem}.md\"\n",
        "    if os.path.exists(output_path): # check if we aleary processed this image\n",
        "        print(f\"File {output_path} already exists\")\n",
        "    else:\n",
        "        print(f\"Processing {image_path.name}...\")\n",
        "        base64_image = encode_image_to_base64(str(image_path)) # encode the image\n",
        "        markdown_content = image_to_markdown(base64_image) # process the image\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(markdown_content) #write the description\n",
        "            print(f\"Markdown for {image_path.name} saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-ZEH5iUA4l"
      },
      "source": [
        "#Step 4 - Create our DataBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ZNn0V5UOTV"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils  import embedding_functions\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(\"/content/.chromadb\")\n",
        "# embedding function\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name=\"pdf\", embedding_function=default_ef)\n",
        "print(\"created new collection\")\n",
        "\n",
        "dataDirectory = \"/content/md_docs/\"\n",
        "\n",
        "for filename in os.listdir(dataDirectory): # Iterate through the files in md_docs/\n",
        "  if filename.endswith('.md'):\n",
        "    # Open the file and read its contents\n",
        "    filepath = os.path.join(dataDirectory, filename)\n",
        "    with open(filepath, 'r') as f:\n",
        "        fileContent = f.read()\n",
        "\n",
        "    # Call your function on the file content\n",
        "    print(f\"adding document {filepath}\")\n",
        "    collection.upsert(documents=[fileContent], ids=[filepath])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE-6keerUVJl"
      },
      "outputs": [],
      "source": [
        "collection.peek(limit=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEe2uZOwqNwu"
      },
      "source": [
        "# Step 5 - Sample Question + Query the relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZO5SFXBUc0b"
      },
      "outputs": [],
      "source": [
        "#### Step 5.1: Question\n",
        "question = \"What were the sport in the ancient olympics?\" #@param {type:\"string\"}\n",
        "nr_of_results = 6 #@param {type:\"slider\", min:1, max:10}\n",
        "#### Step 5.2: Search Vector Database\n",
        "results = collection.query(query_texts=[question], n_results=nr_of_results)\n",
        "# line below just outputs\n",
        "#results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBDsePWqkYa"
      },
      "source": [
        "# Step 6 - Ask our Question to the Large Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBMpKF0ZUwzi"
      },
      "outputs": [],
      "source": [
        "# This let's you format the output nicely!\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLoVIGN9qsfF"
      },
      "source": [
        "## Create our prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI5ocafR-1yb"
      },
      "source": [
        "**Create your Prompt**\n",
        "\n",
        "- <u>**System:**</u> Sets the behaviour of the assistant, the style of the response\n",
        "- <u>**User:**</u> Contains our request\n",
        "- <u>**Assistant:**</u> Can contains response example or history of the previous messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhDqn73CU803"
      },
      "outputs": [],
      "source": [
        "### Now tell the language model how to act. This is an example. (it might be a rather dumm instruction given our context!)\n",
        "\n",
        "# System Prompt Behaviour\n",
        "behaviour = f\"\"\"\\\n",
        "You are a helpful assistant.\n",
        "\"\"\"\n",
        "\n",
        "# Question + Context\n",
        "### This is where we add our question ⬇⬇\n",
        "question = f\"\"\"\\\n",
        "CONTEXT: {results['documents'][0]}\n",
        "\n",
        "QUESTION: {question}\n",
        "\"\"\"\n",
        "\n",
        "### This is the final prompt\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": behaviour},\n",
        "    {\"role\": \"user\", \"content\": question},\n",
        "]\n",
        "## show messages by uncommenting\n",
        "#messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c-C6n5bU_qn"
      },
      "outputs": [],
      "source": [
        "### Call to OPEN AI\n",
        "\n",
        "# randomness seed (0 no randomness)\n",
        "temperature = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "# tokens to expect as response\n",
        "max_tokens = 1500 # @param {type: \"slider\", min:100, max: 12000, step: 100}\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    top_p=1,\n",
        "    frequency_penalty=1,\n",
        "    presence_penalty=1\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
