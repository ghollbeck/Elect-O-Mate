{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Mc5VX61Qt2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wAxTc4L0aeS"
      },
      "outputs": [],
      "source": [
        "!pip install -qU bs4 tiktoken openai langchain\n",
        "!pip install pinecone-client[grpc]\n",
        "#!pip install pinecone-client\n",
        "#!pip install protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IcmAB6Cwtpj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "chat_api_list = [\"Add you API KEY for OPENAI\",]\n",
        "os.environ[\"OPENAI_API_KEY\"] = chat_api_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9BqDMCRgTZW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# initialize openai\n",
        "#openai.api_key = key  #platform.openai.com\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "\n",
        "res = openai.embeddings.create(\n",
        "    input=[\n",
        "        \"Climate Bert Team\",\n",
        "        \"there will be several texts in each batch\"\n",
        "    ], model=embed_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjHmhbBlgTEW"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "indexes = [     # index configurations here...\n",
        "\n",
        "    {\n",
        "        \"name\": \"alldatabases-fact-cheker\",\n",
        "        \"api_key\": \"Ask Authors for API Keys\",\n",
        "        \"environment\": \"us-east4-gcp\",\n",
        "        \"info\" : \"FactChecking\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"alldatabases-fact-cheker\",\n",
        "        \"api_key\": \"Ask Authors for API Keys\",\n",
        "        \"environment\": \"eu-west1-gcp\",\n",
        "        \"info\" : \"FactChecking\"\n",
        "    }\n",
        "]\n",
        "\n",
        "selected_indexes = indexes[0]\n",
        "final_res = []\n",
        "\n",
        "index_name = selected_indexes['name']\n",
        "\n",
        "# Initialize Pinecone connection for the selected index\n",
        "pinecone.init(\n",
        "    api_key=selected_indexes[\"api_key\"],\n",
        "    environment=selected_indexes[\"environment\"],\n",
        ")\n",
        "\n",
        "# Check if the index exists; if not, create it\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='dotproduct'\n",
        "    )\n",
        "\n",
        "# Connect to the index and get stats\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "index.describe_index_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN8NwemXyuID"
      },
      "outputs": [],
      "source": [
        "selected_indexes = indexes[1]\n",
        "final_res = []\n",
        "index_name2 = selected_indexes['name']\n",
        "\n",
        "# Initialize Pinecone connection for the selected index\n",
        "pinecone.init(\n",
        "    api_key=selected_indexes[\"api_key\"],\n",
        "    environment=selected_indexes[\"environment\"],\n",
        ")\n",
        "\n",
        "# Check if the index exists; if not, create it\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name2,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='dotproduct'\n",
        "    )\n",
        "\n",
        "# Connect to the index and get stats\n",
        "index2 = pinecone.GRPCIndex(index_name2)\n",
        "index2.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3RelPWMy4Zq"
      },
      "outputs": [],
      "source": [
        "def get_augmented_query(results, org_name, query, threshold=0.75):\n",
        "    contexts = []\n",
        "    pages = []\n",
        "    filenames = []\n",
        "    urls = []\n",
        "    orgs = []\n",
        "\n",
        "    for res in results:\n",
        "        for item in res['matches']:\n",
        "            if item['score'] >= threshold:\n",
        "                contexts.append(item['metadata']['text'])\n",
        "                pages.append(item['metadata'].get('page', 'N/A'))  # Default to 'N/A' if 'page' does not exist\n",
        "                filenames.append(item['metadata'].get('filename', 'N/A'))  # Default to 'N/A' if 'filename' does not exist\n",
        "                urls.append(item['metadata'].get('url', 'N/A'))  # Default to 'N/A' if 'url' does not exist\n",
        "                orgs.append(item['metadata'].get('org', 'N/A'))\n",
        "\n",
        "    user_info = []\n",
        "    for i in range(len(contexts)):\n",
        "        user_info.append(str(contexts[i]) + \"\\nReference:\" + str(filenames[i]) + \"\\nPage:\" + str(pages[i]) + \"\\nORG:\" + str(orgs[i]) + \"\\nURL:\" + str(urls[i]) + \"\\n\\n---------------\\n\\n\")\n",
        "\n",
        "    augmented_query = \"\\n\".join(user_info)\n",
        "\n",
        "    return augmented_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_aVK5UOCiHu"
      },
      "source": [
        "### Call our API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmVPpifBTPB8"
      },
      "outputs": [],
      "source": [
        "Q1= \"Is it still possible to limit warming to 1.5°C?\"\n",
        "Q2= \"When will we reach 1.5°C?\"\n",
        "Q3= \"What does overshoot mean?\"\n",
        "Q4= \"Can we avoid overshooting 1.5°C?\"\n",
        "Q5= \"Have emissions reductions fallen for some countries?\"\n",
        "Q6= \"What are the issues with financing adaptation?\"\n",
        "Q7= \"Where is the majority of climate finance going?\"\n",
        "Q8= \"What are the options for scaling up adaptation and mitigation in developing countries?\"\n",
        "Q9= \"Which regions will be disproportionally affected by climate change?\"\n",
        "Q10= \"What is climate justice?\"\n",
        "Q11= \"What is maladaptation?\"\n",
        "Q12= \"Is there evidence of maladaptation?\"\n",
        "Q13= \"Will glaciers in Scotland melt?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qdwt-Ah30k2"
      },
      "outputs": [],
      "source": [
        "### K is a hyperparameter, try k=10, 20, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1wh3mCugS8i"
      },
      "outputs": [],
      "source": [
        "user_query = Q10\n",
        "\n",
        "res = openai.embeddings.create(\n",
        "  input=[user_query],\n",
        "  model=embed_model\n",
        ")\n",
        "\n",
        "final_res_WMO = []\n",
        "final_res_IPCC = []\n",
        "final_res_IPCCSYR = []\n",
        "\n",
        "\n",
        "# We query from the selected index and store the results in a temporary variable and then make augmented query\n",
        "xq = res.data[0].embedding\n",
        "\n",
        "temp_res_WMO = index2.query(xq,\n",
        "                      filter={\"org\": \"WMO\"},\n",
        "                      top_k=5,\n",
        "                      include_metadata=True)\n",
        "final_res_WMO.append(temp_res_WMO)\n",
        "\n",
        "temp_res_IPCC = index.query(xq,\n",
        "                      filter={\"org\": \"IPCC\"},\n",
        "                      top_k=5,\n",
        "                      include_metadata=True)\n",
        "final_res_IPCC.append(temp_res_IPCC)\n",
        "\n",
        "\n",
        "\n",
        "temp_res_IPCCSYR = index2.query(xq,\n",
        "                      filter={\"org\": \"IPCC-SYR\"},\n",
        "                      top_k=5,\n",
        "                      include_metadata=True)\n",
        "final_res_IPCCSYR.append(temp_res_IPCCSYR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "augmented_query_WMO = get_augmented_query(final_res_WMO, 'WMO', user_query)\n",
        "augmented_query_IPCC = get_augmented_query(final_res_IPCC, 'IPCC', user_query)\n",
        "augmented_query_IPCCSYR = get_augmented_query(final_res_IPCCSYR, 'IPCC-SYR', user_query)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5vkt4_U7eK"
      },
      "source": [
        "### Hybrid ChatClimate / ChatBiodiversity (please modify the primer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voxlSxXlnqN_"
      },
      "outputs": [],
      "source": [
        "primer = f\"\"\"You are a Q&A bot, an intelligent system that answers user questions based on the information provided by\n",
        "the user above the question and your in-house knowledge. There are five pieces of extra information above the user question.\n",
        "The user question is in the final line. When you use the user information, always indicate the Page and Reference in your answer.\n",
        "The Page and Reference are located below each piece of information. Additionally, let us know which part of your answer is from the IPCC\n",
        "information and which part is based on your in-house knowledge by writing either (IPCC AR6) or (In-house knowledge).\n",
        "If the information cannot be found in the information provided by the user or your in-house knowledge, please say 'There is not enough information'.\"\"\"\n",
        "\n",
        "res_IPCCSYR = openai.chat.completions.create(\n",
        "\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_IPCCSYR + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "res_IPCCALL = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_IPCC + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "res_WMO = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_WMO + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1rrZtGCtZ7D"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res_IPCCSYR.choices[0].message.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLS-5smJVnUE"
      },
      "outputs": [],
      "source": [
        "display(Markdown(res_IPCCALL.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9cRgDV3VnQb"
      },
      "outputs": [],
      "source": [
        "display(Markdown(res_WMO.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poFu0WnKVMYF"
      },
      "source": [
        "###Standalone chatclimate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiT3YC0DyWsR"
      },
      "outputs": [],
      "source": [
        "\n",
        "primer = f\"\"\"You are a Q&A bot, an intelligent system that answers user questions ONLY based on the information provided by\n",
        "the user above the question and NEVER USE your in-house knowledge. There are five pieces of extra information above the user question.\n",
        "The user question is in the final line. When you use the user information, TRY to USE as much as information you can in your answer. We need a comprehensive answer. Always indicate the Page and Reference in your answer.\n",
        "The Page and Reference are located below each piece of information.\n",
        "If the information cannot be found in the information provided by the user, please say 'There is not enough information'.\"\"\"\n",
        "\n",
        "res_IPCCSYR = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_IPCCSYR + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "res_IPCCALL = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_IPCC + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "res_WMO = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query_WMO + \"\\n\\n\" + user_query}\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAGSzQ9Adio4"
      },
      "outputs": [],
      "source": [
        "display(Markdown(res_IPCCSYR.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NxHGCwOdlo3"
      },
      "outputs": [],
      "source": [
        "display(Markdown(res_IPCCALL.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oEgNoFsdldE"
      },
      "outputs": [],
      "source": [
        "display(Markdown(res_WMO.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2fTmLVrV5YO"
      },
      "source": [
        "### GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtxRItcwtmrC"
      },
      "outputs": [],
      "source": [
        "res = openai.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    temperature = 0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": user_query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#another implementation but not from Chatclimate:\n",
        "\n",
        "\n",
        "# from langchain.document_loaders import TextLoader\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.text_splitter import CharacterTextSplitter\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# import os\n",
        "# from getpass import getpass\n",
        "\n",
        "# OPENAI_API_KEY = getpass()\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "# persist_directory = \"db\"\n",
        "# db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "# # -------------------------- Adding spacex_wiki.txt -------------------------- #\n",
        "# loader = TextLoader(\"this_has_to_work/spacex_wiki.txt\", encoding=\"utf8\")\n",
        "# documents = loader.load()\n",
        "# docs = text_splitter.split_documents(documents)\n",
        "# db.add_documents(docs)\n",
        "\n",
        "# # ------------------------- Adding imploson_wiki.txt ------------------------- #\n",
        "# loader = TextLoader(\"this_has_to_work/implosion_wiki.txt\", encoding=\"utf8\")\n",
        "# documents = loader.load()\n",
        "# docs = text_splitter.split_documents(documents)\n",
        "# db.add_documents(docs)\n",
        "\n",
        "# db.persist()\n",
        "\n",
        "# # --------------------------- querying the vectordb -------------------------- #\n",
        "# db = None\n",
        "\n",
        "# db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "# retriever = db.as_retriever(search_type=\"mmr\")\n",
        "\n",
        "# query = \"What is implosion?\"\n",
        "# print(query)\n",
        "# print(retriever.get_relevant_documents(query)[0])\n",
        "# print(\"\\n\\n\")\n",
        "\n",
        "# query = \"Who is elon?\"\n",
        "# print(query)\n",
        "# print(retriever.get_relevant_documents(query)[0])\n",
        "# print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
